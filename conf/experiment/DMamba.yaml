torch_dataset_name: temp_dataset  # temp_dataset or vel_dataset

distributed: False

train:
  max_epochs: 1
  batch_size: 8
  shuffle_data: True
  time_window: 5
  future_window: 5
  push_forward_steps: 1
  use_coords: False
  noise: True
  downsample_factor: 1

seed: 42
device: cuda:0
verbose: True
log_to_wandb: False
wandb:
  project: 'BubbleML'
  name: 'DMamba'
  api_key: 'e9f2308aef4f28fc8bbc3231e04370d8528cb993'

model:
  model_name: 'DMamba'  # this should be the same as the model class name
  model_num: 1000
  time_window: 5
  push_forward_steps: 1
  input_size: [5, 512, 512]
  patch_size: [1, 16, 16]
  stride: [1, 8, 8]
  channels: 7
  dim: 32
  mlp_ratio: 4
  patch_size_seq: [[1, 16, 16]]
  patch_size_probs: null
  depth: 3
  dropout: 0
  dropout_embed: 0


optimizer:
  initial_lr: 1e-3
  weight_decay: 0.01

lr_scheduler:
  name: step
  factor: 0.5
  patience: 75
